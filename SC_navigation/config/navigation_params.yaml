training: #namespace

    hidden1: 400            #hidden num of first fully connect layer
    hidden2: 300            #hidden num of second fully connect layer 
    rate: 0.0001             #learning rate
    prate: 0.0001           #policy net learning rate (only for DDPG)
    warmup: 10             #time without training but only filling the replay memory
    discount: 0.99          
    bsize: 8               #minibatch size
    rmsize: 1000 #6000000         #memory size
    window_length: 1         
    tau: 0.001              #moving average for target network
    ou_theta: 0.15          #noise theta
    ou_sigma: 1.2           #noise sigma
    ou_mu: 0                #noise mu
    validate_episodes: 20   #how many episode to perform during validate experiment
    max_episode_length: 500
    validate_steps: 2000    #how many steps to perform a validate experiment
    output: 'output'        #where to save the stuff
    parent_dir: '/home/adriano/Desktop/thesis_ws/src/Shared_control_architecture'
    debug: 'debug'
    init_w: 0.003
    train_iter: 200000      #'train iters each timestep
    epsilon_decay: 0.995
    epsilon:  0.6          #linear decay of exploration policy
    max_epochs: 100
    seed: -1
    delta_goal: 0.4
    n_frame: 4

rewards:
    R_safe: -5 #
    R_col: -1000
    R_goal: -10
    R_end: 1000
    R_alpha: -10 #
    R_cmd: -10
  

controller:
    #mode: 'train'           #support option: train/test/direct
    rate: 10
    usr_rate: 5
    verbose: False
    env: 'static' #_complex' #'office_NO_walls'  #'inventory_intensive'        #which is the world you are using


    #for collision avoidance
    delta_coll: 1.0
    K_lin: 1 #0.5
    K_ang: 1 #0.5

    #tiago
    taigoMBclear: 0.6   #the rad is approx 0.25 and the obstacles are 0.2

    poly_degree: 2
    n_actions: 3
    dt: 0.1









